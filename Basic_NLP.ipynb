{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJkrqDUPu8d4PLPkVuLg4D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhkr1/Practical-Al-Natural-Language-Processing/blob/main/Basic_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 전처리(Text PreProcessing)"
      ],
      "metadata": {
        "id": "y__CWWE5CLml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) 토큰화(Tokenizing)"
      ],
      "metadata": {
        "id": "sVcVG5ypCPU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. NLTK 라이브러리를 3.8.1 버전으로 강제 재설치합니다.\n",
        "!pip install --force-reinstall nltk==3.8.1\n",
        "\n",
        "# 2. 변경된 버전을 적용하기 위해 런타임을 강제로 재시작합니다.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7IjnSyu98GX",
        "outputId": "07cd2167-e4b2-449d-bf29-a1d1ddfc5ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting click (from nltk==3.8.1)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk==3.8.1)\n",
            "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk==3.8.1)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from nltk==3.8.1)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.1\n",
            "    Uninstalling joblib-1.5.1:\n",
            "      Successfully uninstalled joblib-1.5.1\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.2.1 joblib-1.5.1 nltk-3.8.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 다운그레이드된 NLTK를 위해 punkt 데이터를 다운로드합니다.\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is the very last attempt with a stable NLTK version 3.8.1.\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"--- 특정 버전(3.8.1) 설치 후 최종 테스트 ---\")\n",
        "print(\"성공! 토큰화 결과:\")\n",
        "print(tokens)\n",
        "\n",
        "# 실제로 버전이 3.8.1로 변경되었는지 확인합니다.\n",
        "import nltk\n",
        "print(\"\\n설치된 NLTK 버전:\", nltk.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5YGnwHa-BMT",
        "outputId": "dad83a0e-8831-46ec-b999-c565489766ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 특정 버전(3.8.1) 설치 후 최종 테스트 ---\n",
            "성공! 토큰화 결과:\n",
            "['This', 'is', 'the', 'very', 'last', 'attempt', 'with', 'a', 'stable', 'NLTK', 'version', '3.8.1', '.']\n",
            "\n",
            "설치된 NLTK 버전: 3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "text = \"Barak Obama likes fried chicken very much\"\n",
        "wordpuncttoken = WordPunctTokenizer().tokenize(text)\n",
        "print(wordpuncttoken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb9wOl_k_u2I",
        "outputId": "3ca0c008-b4f9-4f86-cda1-81f9920ca96d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barak', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "text = \"Barak Obama likes fried chicken very much\"\n",
        "TreebankWordToken = TreebankWordTokenizer().tokenize(text)\n",
        "print(TreebankWordToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaEGI-cMAI1X",
        "outputId": "c321ee4b-4e2c-450e-b107-e6a8d85593fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barak', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 영문 품사 부착(PoS Tagging)"
      ],
      "metadata": {
        "id": "Wkfc6Fl8ASr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRMOt02NAYFJ",
        "outputId": "c3ea1408-719d-4d0d-8178-7442cd1d5fbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taggedToken = pos_tag(wordpuncttoken)\n",
        "print(taggedToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAmqR3iXAmUl",
        "outputId": "fcfdf54c-5c97-477f-e9b9-8df890b01b09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Barak', 'NNP'), ('Obama', 'NNP'), ('likes', 'VBZ'), ('fried', 'VBN'), ('chicken', 'JJ'), ('very', 'RB'), ('much', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) 개채명 인식(NER, Named Entity Recognition)"
      ],
      "metadata": {
        "id": "Ev5s25SZA344"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3dFUHWtBATH",
        "outputId": "60856437-94d3-40e8-e2f9-d559692e1cbd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx_uMnFRBTsM",
        "outputId": "3da66c5c-03e6-4891-bfe9-675f22f75ef9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Barak/NNP)\n",
            "  (ORGANIZATION Obama/NNP)\n",
            "  likes/VBZ\n",
            "  fried/VBN\n",
            "  chicken/JJ\n",
            "  very/RB\n",
            "  much/JJ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) 원형 복원"
      ],
      "metadata": {
        "id": "ugx3gNqxBeZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-1) 어간추출(Stemming)\n"
      ],
      "metadata": {
        "id": "pakoFhqbBiF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print('running ->', ps.stem('running'))\n",
        "print('beautiful ->', ps.stem('beautiful'))\n",
        "print('believes ->', ps.stem('believes'))\n",
        "print('using ->', ps.stem('using'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPYRE280Bgka",
        "outputId": "678589c6-9fe2-4419-ae5f-64aafbea08c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> run\n",
            "beautiful -> beauti\n",
            "believes -> believ\n",
            "using -> use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4-2) 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "D_nYWJnpoDZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REN5n-tSoJ1c",
        "outputId": "57681227-c739-4701-c8f9-f6b24ea09dba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "print(\"running -> \" + wl.lemmatize(\"running\"))\n",
        "print(\"beautiful -> \" + wl.lemmatize(\"beautiful\"))\n",
        "print(\"believes -> \" + wl.lemmatize(\"believes\"))\n",
        "print(\"using -> \" + wl.lemmatize(\"using\"))\n",
        "print(\"conversation -> \" + wl.lemmatize(\"conversation\"))\n",
        "print(\"organization -> \" + wl.lemmatize(\"organization\"))\n",
        "print(\"studies -> \" + wl.lemmatize(\"studies\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dioUNxHFoMsu",
        "outputId": "e5a056e2-cf2a-4a3d-ae9a-471f51e66b30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> running\n",
            "beautiful -> beautiful\n",
            "believes -> belief\n",
            "using -> using\n",
            "conversation -> conversation\n",
            "organization -> organization\n",
            "studies -> study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) 불용어 처리(Stopword)"
      ],
      "metadata": {
        "id": "yrv3ezE7oY3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopPos = ['IN', 'CC', 'UH', 'TO', 'MD', 'DT', 'VBZ','VBP']"
      ],
      "metadata": {
        "id": "_cs3FhMvocyE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PhcGYXfoipN",
        "outputId": "cf6a57bb-5ac9-4aab-fb5b-62e262e8a416"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('Barak', 'NNP'), 1),\n",
              " (('Obama', 'NNP'), 1),\n",
              " (('likes', 'VBZ'), 1),\n",
              " (('fried', 'VBN'), 1),\n",
              " (('chicken', 'JJ'), 1),\n",
              " (('very', 'RB'), 1),\n",
              " (('much', 'JJ'), 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopWord = [',', 'be', 'able', 'very']\n",
        "\n",
        "word = []\n",
        "for tag in taggedToken:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu8AFuqIopDs",
        "outputId": "29916619-58f3-4264-e24c-185e74f7c60d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barak', 'Obama', 'fried', 'chicken', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) 영문 텍스트 전처리 종합"
      ],
      "metadata": {
        "id": "Hz_5urMko8EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "text = \"Barak Obama loves fried chicken of KFC\"\n",
        "sumtoken = TreebankWordTokenizer().tokenize(text)\n",
        "print(sumtoken)\n",
        "\n",
        "from nltk import pos_tag\n",
        "sumTaggedToken = pos_tag(sumtoken)\n",
        "print(sumTaggedToken)\n",
        "\n",
        "from nltk import ne_chunk\n",
        "sumNeToken = ne_chunk(sumTaggedToken)\n",
        "print(sumNeToken)\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(\"loves -> \" + ps.stem(\"loves\"))\n",
        "print(\"fried -> \" + ps.stem(\"fried\"))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "print(\"loves -> \" + wl.lemmatize(\"loves\"))\n",
        "print(\"fried -> \" + wl.lemmatize(\"fried\"))\n",
        "\n",
        "\n",
        "# 불용어 처리\n",
        "sumStopPos = ['IN']\n",
        "sumStopWord = ['fried']\n",
        "\n",
        "word = []\n",
        "for tag in sumTaggedToken:\n",
        "  if tag[1] not in sumStopPos:\n",
        "    if tag[0] not in sumStopWord:\n",
        "      word.append(tag[0])\n",
        "print(word)"
      ],
      "metadata": {
        "id": "SST-4yWwo_yF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f99964-6fd5-4833-bfce-1700e0648e6e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barak', 'Obama', 'loves', 'fried', 'chicken', 'of', 'KFC']\n",
            "[('Barak', 'NNP'), ('Obama', 'NNP'), ('loves', 'VBZ'), ('fried', 'VBN'), ('chicken', 'NN'), ('of', 'IN'), ('KFC', 'NNP')]\n",
            "(S\n",
            "  (PERSON Barak/NNP)\n",
            "  (ORGANIZATION Obama/NNP)\n",
            "  loves/VBZ\n",
            "  fried/VBN\n",
            "  chicken/NN\n",
            "  of/IN\n",
            "  (ORGANIZATION KFC/NNP))\n",
            "loves -> love\n",
            "fried -> fri\n",
            "loves -> love\n",
            "fried -> fried\n",
            "['Barak', 'Obama', 'loves', 'chicken', 'KFC']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 한글 전처리 실습"
      ],
      "metadata": {
        "id": "xS8W0amP052O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "영문은 공백으로 토큰화가 가능하지만, 한글의 경우 품사를 고려하여 토큰화 해야한다."
      ],
      "metadata": {
        "id": "rhBOIACg08IB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) 한글 토큰화 및 형태소 분석"
      ],
      "metadata": {
        "id": "h-A1k4s_09Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f73pi-M0-2e",
        "outputId": "078c572a-d8ee-42f6-b6e2-1852a3d49547"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란(Komoran) 토큰화\n",
        "from konlpy.tag import Komoran\n",
        "komoran= Komoran()\n",
        "kor_text = '인간이 컴퓨터와 대화하고 있다는 것을 깨닫지 못한다면 컴퓨터는 지능적인 것으로 간주가 가능하다.'\n",
        "komoran_tokens = komoran.morphs(kor_text)\n",
        "print(komoran_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-HBS7082Aa5",
        "outputId": "47a82414-2a24-4582-b1c9-f9e721e44192"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', 'ㄴ다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '가', '가능', '하', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한나눔 토큰화\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "hannanum_tokens = hannanum.morphs(kor_text)\n",
        "print(hannanum_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7FRcEat1H8C",
        "outputId": "1ba2d653-7814-44bb-ecb8-7cbde1006d72"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있', '다는', '것', '을', '깨닫', '지', '못하', 'ㄴ다면', '컴퓨터', '는', '지능적', '이', 'ㄴ', '것', '으로', '간주', '가', '가능', '하', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt 토큰화\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "okt_tokens = okt.morphs(kor_text)\n",
        "print(okt_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zju29-RD1eU5",
        "outputId": "e02db735-2b97-4549-dea3-6de19ff61142"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하고', '있다는', '것', '을', '깨닫지', '못', '한다면', '컴퓨터', '는', '지능', '적', '인', '것', '으로', '간주', '가', '가능하다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma_tokens = kkma.morphs(kor_text)\n",
        "print(kkma_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kaw2QtD91nIo",
        "outputId": "164a6048-c3f2-40a1-866e-86a742ea2c58"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '이', '컴퓨터', '와', '대화', '하', '고', '있', '다는', '것', '을', '깨닫', '지', '못하', 'ㄴ다면', '컴퓨터', '는', '지능', '적', '이', 'ㄴ', '것', '으로', '간주', '가', '가능', '하', '다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) 한글 품사 부착(PoS Tagging)"
      ],
      "metadata": {
        "id": "4uBP8YBY12Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코모란(Komoran) 품사 태깅\n",
        "komoranTag = []\n",
        "for token in komoran_tokens:\n",
        "    komoranTag += komoran.pos(token)\n",
        "print(komoranTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0CJMYGJ15LZ",
        "outputId": "3f1f824d-23a0-449f-8315-3daf0a7a19cf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'MM'), ('컴퓨터', 'NNG'), ('오', 'VV'), ('아', 'EC'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'MM'), ('있', 'VV'), ('달', 'VV'), ('는', 'ETM'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNB'), ('못', 'MAG'), ('하', 'MAG'), ('ㄴ다면', 'EC'), ('컴퓨터', 'NNG'), ('늘', 'VV'), ('ㄴ', 'ETM'), ('지능', 'NNP'), ('적', 'NNB'), ('이', 'MM'), ('ㄴ', 'JX'), ('것', 'NNB'), ('으로', 'JKB'), ('간주', 'NNG'), ('가', 'VV'), ('아', 'EC'), ('가능', 'XR'), ('하', 'NNG'), ('다', 'MAG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hannanumTag = []\n",
        "for token in hannanum_tokens:\n",
        "    hannanumTag += hannanum.pos(token)\n",
        "print(hannanumTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5D0zqki2K7g",
        "outputId": "337a0920-fdeb-48b6-db42-909fd3b0097e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'N'), ('이', 'M'), ('컴퓨터', 'N'), ('와', 'I'), ('대화', 'N'), ('하', 'P'), ('고', 'E'), ('있', 'N'), ('다', 'M'), ('는', 'J'), ('것', 'N'), ('을', 'N'), ('깨닫', 'N'), ('지', 'N'), ('못하', 'P'), ('어', 'E'), ('ㄴ다', 'N'), ('이', 'J'), ('면', 'E'), ('컴퓨터', 'N'), ('늘', 'P'), ('ㄴ', 'E'), ('지능적', 'N'), ('이', 'M'), ('ㄴ', 'N'), ('것', 'N'), ('으', 'N'), ('로', 'J'), ('간주', 'N'), ('가', 'J'), ('가능', 'N'), ('하', 'I'), ('다', 'M'), ('.', 'S')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt 품사 태깅\n",
        "oktTag = []\n",
        "for token in okt_tokens:\n",
        "    oktTag += okt.pos(token)\n",
        "print(oktTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCjRSkJq2Poi",
        "outputId": "25f38a54-03ca-42d7-f100-a8821ff5b379"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'Noun'), ('이', 'Noun'), ('컴퓨터', 'Noun'), ('와', 'Verb'), ('대화', 'Noun'), ('하고', 'Verb'), ('있다는', 'Adjective'), ('것', 'Noun'), ('을', 'Josa'), ('깨닫지', 'Verb'), ('못', 'Noun'), ('한다면', 'Verb'), ('컴퓨터', 'Noun'), ('는', 'Verb'), ('지능', 'Noun'), ('적', 'Noun'), ('인', 'Noun'), ('것', 'Noun'), ('으로', 'Josa'), ('간주', 'Noun'), ('가', 'Verb'), ('가능하다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kkma 품사 태깅\n",
        "kkmaTag = []\n",
        "for token in kkma_tokens:\n",
        "    kkmaTag += kkma.pos(token)\n",
        "print(kkmaTag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFcmY4lz2Rj-",
        "outputId": "0b0ebbb5-9bac-4913-b585-093cb98fbc13"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('인간', 'NNG'), ('이', 'NNG'), ('컴퓨터', 'NNG'), ('오', 'VA'), ('아', 'ECS'), ('대화', 'NNG'), ('하', 'NNG'), ('고', 'NNG'), ('있', 'VA'), ('달', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'NNG'), ('깨닫', 'VV'), ('지', 'NNG'), ('못하', 'VX'), ('ㄴ다면', 'UN'), ('컴퓨터', 'NNG'), ('늘', 'VA'), ('ㄴ', 'ETD'), ('지능', 'NNG'), ('적', 'NNG'), ('이', 'NNG'), ('ㄴ', 'NNG'), ('것', 'NNB'), ('으', 'UN'), ('로', 'JKM'), ('간주', 'NNG'), ('가', 'NNG'), ('가능', 'NNG'), ('하', 'NNG'), ('다', 'NNG'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) 불용어(Stopword) 처리"
      ],
      "metadata": {
        "id": "xI0otv4p2TrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 불용어 처리\n",
        "stopPos = ['Suffix', 'Punctuation', 'Josa','Foreign','Alpha','Number']\n",
        "\n",
        "# 최빈어 조회. 최빈어를 조회하여 불용어 제거 대상을 선정\n",
        "from collections import Counter\n",
        "Counter(oktTag).most_common()\n",
        "\n",
        "stopWord = ['의','이','로','두고','들','를','은','과','수','했다','것','있는','한다','하는','그','있다','할','이런','되기','해야','있게','여기']\n",
        "\n",
        "word = []\n",
        "for tag in oktTag:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EeczId92W90",
        "outputId": "50dcc564-4c57-4d8e-b376-745dcd8f8e5f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['인간', '컴퓨터', '와', '대화', '하고', '있다는', '깨닫지', '못', '한다면', '컴퓨터', '는', '지능', '적', '인', '간주', '가', '가능하다']\n"
          ]
        }
      ]
    }
  ]
}